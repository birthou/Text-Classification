{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification de textes & Word Embeddings\n",
    "\n",
    "                                            Emanuela Boros\n",
    "                                            Université de La Rochelle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "La classification des objets consiste à attribuer une classe à un objet. Ce objet peut être de type texte, image, audio ou vidéo. Nous faisons de la classification tout le temps. Par exemple, un chaffeur de voitures a besoin de reconaître tous les objets dans la rue pour pouvoir prévoir sa trajectoire. Il est importante de faire la différence entre un cycliste et un piéton, une moto et une voiture, les différents types de panneaux pour savoir où il peut aller, une personne et un policier, etc.\n",
    "\n",
    "Pour savoir comment classifier un objet, il est important de connaître les caracteristiques qui definissent une classe. Par exemple, si on considere les caracteristiques : quantités de roues, selle, guidon, et frein. Un vélo ordinaire est composé de deux roues, une selle et un guidon. Une voiture a 4 roues, des freins mais pas de selle ni cadre ni guidon. Une moto est aussi composée de deux roues, une selle, des friens et un guidon. Avec ces caracteristiques, la moto et le vélo ont la même répresentation.\n",
    "\n",
    "Nous pourrions compliquer encore plus cette tâche et essayer de classifier les chihuahuas ou muffins suivants. Pour faire cela, quelles sont les caracteristiques pertinentes à analyser ?\n",
    "\n",
    "<figure>\n",
    "    <center>\n",
    "    <img src=\"images/classification.jpeg\" width=\"400\" height=\"600\">\n",
    "    <figcaption>Source: <a href=\"https://www.freecodecamp.org/news/chihuahua-or-muffin-my-search-for-the-best-computer-vision-api-cbda4d6b425d/\">Link</a></figcaption>\n",
    "    </center>\n",
    "</figure>\n",
    "\n",
    "Vous trouvez ça difficile ? Vous avez raison.\n",
    "\n",
    "La quantité et la pertinence des caracteristiques peuvent aider à classifier des objets. Plus les caracteristiques sont indépendantes et différentes, plus on aura d'informations complementaires sur l'objet analysé.\n",
    "\n",
    "En suivant cette méthode, nous analyserons dans ce cours la classification de textes. Il est très important de faire attention aux caracteristiques pour chaque objet (dans ce cas : les mots et leurs caractéristiques). Notre objectif est d'avoir des caracteristiques pertinentes pour mieux définir un objet et pouvoir l'attribuer à une catégorie correctement.\n",
    "\n",
    "\n",
    "<figure>\n",
    "    <center>\n",
    "    <img src=\"images/text_processing_flow.png\" width=\"700\" height=\"100\">\n",
    "    <figcaption>Source: <a href=\"https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/\">Link</a></figcaption>\n",
    "    </center>\n",
    "</figure>\n",
    "\n",
    "\n",
    "Dans ce cours, nous explorerons un ensemble de données, une analyse de texte et plusieurs approches basées sur l'apprentissage automatique et profond, de la manière suivante:\n",
    "\n",
    "- **Récupération** du corpus de textes (*.csv, *.txt, *.json, etc.)\n",
    "- **Prétraitement** des données textuelles (text pre-processing) : tokenisation etc\n",
    "- **Exploration** du corpus (*EDA*, exploratory data analysis) : analyse des fréquences\n",
    "- **Representation** des mots (bag of words, *sac de mots*, TF-IDF, plongements de mots, *word embeddings*)\n",
    "- **Apprentissage automatique** (machine learning) et **apprentissage profond** (deep learning)\n",
    "- **Analyse d'erreurs** (evaluation, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Récupération, Prétraitement et Exploration du corpus de textes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 1. Contexte\n",
    "\n",
    "Le gouvernement américain a attaqué en justice cinq grands groupes américains du tabac pour avoir amassé d'importants bénéfices en mentant sur les dangers de la cigarette. Le cigarettiers  se sont entendus dès 1953, pour \"mener ensemble une vaste campagne de relations publiques afin de contrer les preuves de plus en plus manifestes d'un lien entre la consommation de tabac et des maladies graves\". \n",
    "\n",
    "Dans ce procès 6,910,192 de documents ont été collectés et numérisés. Afin de faciliter l'exploitation de ces documents par les avocats, vous êtes en charge de mettre en place une classification automatique des types de documents: **Advertisement, Email, Form, Letter, Memo, News, Note, Report, Resume, Scientific**.\n",
    "\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>Letter</td>\n",
    "     <td>News</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img src=\"images/50661869-1869.jpg\" width=270 height=480></td>\n",
    "    <td><img src=\"images/10031617.jpg\" width=270 height=480></td>\n",
    "  </tr>\n",
    " </table>\n",
    "\n",
    "\n",
    "Un échantillon aléatoire des documents a été collecté et des opérateurs ont classé les documents dans des répertoires correspondant aux classes de documents : lettres, rapports, notes, email, etc. Vous avez à votre disposition : \n",
    "\n",
    "- le texte contenu dans les documents obtenu par OCR (en anglais : optical character recognition; signifie reconnaissance optique de caractères ou reconnaissance de texte, une technologie qui permet de convertir différents types de documents tels que les documents papiers scannés en fichiers modifiables - texte); <ins>**path**</ins>\n",
    "- les classes des documents définies par des opérateurs; <ins>**label**</ins>\n",
    "\n",
    "Aprés téléchargement et extraction du fichier compressé, nous pourrons charger et regarder le structure de ces commentaires avec la bibliothèque pandas (plus d'info <a href=\"https://ghajba.developpez.com/tutoriels/python/analyse-donnees-avec-pandas/\">ici</a>).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # https://pandas.pydata.org/docs/\n",
    "\n",
    "data = pd.read_csv('data/Tobacco3482-text/tobacco_texts.csv') # lire le fichier .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head() # afficher les 5 premières lignes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail() # afficher les 5 dernières lignes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec Pandas, nous pouvons commencer avec quelques statistiques simples. Les résultats suivants seront dans l'ordre décroissant de sorte que le premier élément soit l'élément le plus fréquent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec Pandas, nous pouvons également visualiser ces statistiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = data.label.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons vu que le fichier .csv contenait les chemins et les étiquettes des documents. Maintenant, nous devons lire les fichiers à partir des chemins spécifiés pour obtenir les données textuelles.\n",
    "\n",
    "**<span style=\"color:red\">To do</span>**:\n",
    "\n",
    "> * Pour chaque chemin dans le dataframe, lisez le fichier associé et ajoutez les textes dans un vecteur. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "texts = []\n",
    "for idx, line in data.iterrows(): # itérer dans un dataframe Pandas\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "data['text'] = texts # créer une nouvelle colonne dans le dataframe Pandas avec les textes associés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head() # visualisez les donnees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemple tiré de l'ensemble de données Tobacco:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'][0], 'Classe :', data['label'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Étant donné que cet ensemble de données a été numérisé, des problèmes peuvent apparaître avec les données. Il n'y a toujours pas d'outils OCR qui fonctionnent au niveau humain dans la plupart des applications. L'existence aujourd'hui de plusieurs outils de ce type a conduit peu à peu à définir des critères de choix pour sélectionner l'OCR le plus efficace et surtout le mieux adapté à son application. Longtemps, le critère d'efficacité était lié à un taux de reconnaissance élevé, pensant qu'une technologie efficace est une technologie sans défaut. En effet, il faut admettre quele taux de 100% reste un objectif à atteindre.\n",
    "\n",
    "Les erreurs incluent une mauvaise lecture des lettres, le saut de lettres illisibles ou la combinaison de texte de colonnes adjacentes ou de légendes d'image. Bien que de nombreux facteurs affectent les performances des outils OCR, le nombre d'erreurs dépend de la qualité et de la forme du texte, y compris du font utilisé.\n",
    "\n",
    "\n",
    "<figure>\n",
    "    <center>\n",
    "    <img src=\"images/1004859787.jpg\" width=\"300\" height=\"100\">\n",
    "    <figcaption>News</figcaption>\n",
    "    </center>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un exemple d'un tel document, avec de nombreuses erreurs de reconnaissance de caractères et de mots :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'][57], 'Classe :', data['label'][57], data['path'][57]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La figure correspond au texte et il est visible que sa qualité est faible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Prétraitement de texte \n",
    "\n",
    "Dans tout processus d'analyse de texte, la première et la plus importante des étapes est de **constituer un corpus**, un ensemble de documents, et de **le nettoyer** afin de l'exploiter. Nous allons réaliser quelques opérations statistiques pour mieux comprendre comment ils sont constitués et présenter les principes de base.\n",
    "\n",
    "Les étapes de prétraitement (*preprocessing*) sont cruciales car elles permettent de **nettoyer le texte** de tous ses éléments qui ne sont **pas porteurs de sens**, et le préparent pour l’analyse.\n",
    "Les différentes étapes sont :\n",
    "1. La ***tokenisation*** est une tâche courante dans le traitement automatique du langage naturel (TALN). Les **tokens** sont les éléments constitutifs du langage naturel. La tokenisation est un moyen de séparer un morceau de texte en unités plus petites appelées **tokens**. Les **tokens** sont généralement des **mots**.\n",
    "\n",
    "2. La ***normalisation*** de texte est le processus de transformation d'un texte en une forme canonique (standard). Par exemple, le mot «gooood» et «gud» peut être transformé en «good», sa forme canonique. La normalisation du texte est importante pour les textes bruyants tels que les commentaires sur les réseaux sociaux, les messages texte et les commentaires sur les articles de blog, et aussi pour les documents numérisés qui, comme nous l'avons vu, contiennent de nombreuses erreurs: les **abréviations**, les **fautes d'orthographe** et l'utilisation de mots hors vocabulaire (oov) sont répandus. Ce procédé implique également la **suppression des mots vides** ou des **mots bruyants** qui peuvent perturber l'analyse (conjugaisons, majuscules, ponctuations, etc.).\n",
    "\n",
    "**Note**: en analyse de données, ces étapes sont généralement les plus fastidieuses, car elle impliquent un long travail de normalisation de données.\n",
    "\n",
    "Tout au long de ce chapitre, nous allons utiliser les bibliothèques de traitement du langage [`nltk`](https://www.nltk.org/), [`spaCy`](https://spacy.io) ainsi que les bibliothèques scientifiques classiques que sont [`pandas`](https://pandas.pydata.org/), [`numpy`](https://numpy.org/) et [`scikit-learn`](https://scikit-learn.org)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans la première partie du cours, nous avons étudié la pertinence du pre-traitement du texte pour extraire les élements le plus pertinents pour décrire ce texte. Certains élements du texte ne sont pas pertinents dans la classification de spams : le masculin et feminin de mots, la conjugaison de verbes, stopwords, singulier et pluriel des mots, ... \n",
    "\n",
    "La bibliothèque [`spaCy`](https://spacy.io) nous permet d'obtenir ces informations facilement.\n",
    "\n",
    "PS: l'installation de cette bibliothèque est très simple dans envirement virtuel (ou ici, dans le notebook):\n",
    "\n",
    "```\n",
    "pip install spacy\n",
    "python -m spacy download en_core_web_sm```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Suppression des [mots vides](https://fr.wikipedia.org/wiki/Mot_vide) (stopwords)\n",
    "\n",
    "La première manipulation souvent effectuée dans le traitement du texte est la suppression des mots vides, ou *stopwords*. Ce sont les mots très courants dans la langue étudiée (« et », « à », « le »… en français) qui dans beaucoup des cas **n'apportent pas de valeur informative** pour la compréhension d’un document ou d’un corpus. \n",
    "\n",
    "Par exemple, dans le contexte d'un système de recherche, si votre requête de recherche est «Qu'est-ce que le prétraitement de texte ?», vous voulez que le système de recherche se concentre sur les documents qui parlent de «prétraitement de texte» plutôt que sur les documents qui parlent de «Qu'est-ce que le». Cela peut être fait en empêchant tous les mots de votre liste de mots vides d'être analysés. Les mots vides sont couramment appliqués dans les systèmes de recherche, les applications de classification de texte, la modélisation de sujets, l'extraction de sujets et autres.\n",
    "\n",
    "Par contre, la suppression des mots vides, bien qu'efficace dans les systèmes de recherche et d'extraction de sujets, s'est avérée non critique dans les algorithmes de classification and ils sont assez importants dans la création d'un [**modèle de langage**](https://medium.com/@pierre_guillou/nlp-fastai-gpt-2-16ee145a4a28) dans lequel le sens d'un mot dépend de tous les mots environnants. Cependant, cela aide à réduire le nombre de fonctionnalités prises en compte, ce qui permet de garder vos modèles de taille décente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\") # charge le modèle en anglais\n",
    "\n",
    "spacy_stopwords = list(spacy.lang.en.stop_words.STOP_WORDS) # la liste des mots vides de SpaCy\n",
    "\n",
    "punctuation = list(string.punctuation) # une liste avec ponctuations\n",
    "\n",
    "spacy_stopwords[:10], punctuation[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici un exemple du fonctionnement de spaCy. Vous pouvez trouver plus de détails ici [`spaCy 101`](https://spacy.io/usage/spacy-101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('Apple is looking at buying U.K. startup for $1 billion.')\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, nous créons une fonction pour prétraiter tous les documents dans le dataframe Pandas. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(phrase):\n",
    "    \n",
    "    tokens = [word.text.replace('\\n', '').strip() for word in nlp(phrase) \n",
    "              if word.text.lower() not in spacy_stopwords + punctuation]\n",
    "    tokens = [token for token in tokens if len(token) > 1]\n",
    "    if len(tokens) > 1:\n",
    "        return tokens\n",
    "    return None\n",
    "\n",
    "data['cleaned_text'] = data['text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head() # visualisez les donnees apres pre-traitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cleaned_text'][0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois que le prétraitement peut supprimer de nombreux mots, il est possible que les documents restent vides. Nous allons vérifier cela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cleaned_text'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il y a des documents sans mots restants, nous les rejetons donc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(subset=['cleaned_text'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérifiez à nouveau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cleaned_text'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['document_length'] = data['cleaned_text'].apply(lambda tokens: len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['document_length'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(data['document_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sort_values('document_length', inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = data.plot(y='document_length', kind='hist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**<span style=\"color:red\">To do</span>**:\n",
    "\n",
    "> * Maintenant que nous avons utilisé [`spaCy`](https://spacy.io) pour le prétraitement, écrivez une fonction qui utilise la bibliothèque [`nltk`](https://www.nltk.org/). \n",
    "> * Utilisez [```word_tokenize```](https://www.nltk.org/api/nltk.tokenize.html) pour la tokenisation, téléchargez les mots vides nltk avec ``python -m nltk.downloader stopwords`` et supprimez tous les autres **tokens** qui, selon vous, nuiront à la précision des algorithmes d'apprentissage automatique (nombres? Ponctuation? Etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "!python -m nltk.downloader stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_with_nltk(phrase):\n",
    "    \n",
    "    tokens = []# YOUR CODE HERE\n",
    "    if len(tokens) > 1:\n",
    "        return tokens\n",
    "    return None\n",
    "\n",
    "data['cleaned_text_nltk'] = data['text'].apply(preprocess_with_nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que nous savons **nettoyer les données**, nous pouvons aussi analyser le corpus avec des autres stratistiques, comme les occurences de chaque mot. Nous utilisons la classe [`nltk.FreqDist`](https://www.nltk.org/api/nltk.html?highlight=freqdist).\n",
    "\n",
    "Plus d'informations pour [`numpy hstack`](https://numpy.org/doc/stable/reference/generated/numpy.hstack.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "freq = nltk.FreqDist(np.hstack(data['cleaned_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in ['cigarettes', 'cancer', 'alcohol', 'death']:\n",
    "    print(word, ':', freq[word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut grâce à `FreqDist` récupérer les 25 termes les plus courants du corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(freq.most_common(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = freq.plot(50, cumulative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Co-occurences\n",
    "\n",
    "En plus du processus de *tokenisation*, il est possible de représenter le texte en groupe de plusieurs mots adjacents. On appelle les groupes de mots les [**n-grammes**](https://fr.wikipedia.org/wiki/N-gramme) : **bigrammes** pour les **couples** de mots, **trigrammes** pour les **groupes de trois**, etc. Séparer en **mots individuels** est en fait un cas particulier appelé **unigramme**.\n",
    "\n",
    "Par exemple, on peut extraire les **bigrammes** :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "list(nltk.ngrams(np.hstack(data['cleaned_text']), 2))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2  Lemmatisation\n",
    "\n",
    "Le processus de « [**lemmatisation**](https://fr.wikipedia.org/wiki/Lemmatisation) » consiste à représenter les mots (ou [lemmes](https://fr.wikipedia.org/wiki/Lemme_(linguistique)) en linguistique) sous leur forme canonique. Par exemple pour un verbe, ce sera son infinitif. Pour un nom, son masculin singulier. On ne **conserve que le sens des mots** utilisés dans le corpus.\n",
    "\n",
    "Si l’on désire étudier la richesse d’un vocabulaire, il est effectivement préférable de compter les occurences de « *be* » plutôt que « *is* » ou « *was* » qui sont des conjugaisons du même verbe, « *to be* ». Il en va de même pour toutes les autres formes verbales, nominales, adjectifs ou adverbes.\n",
    "\n",
    "Le termes **had** et **been** auraient dû être respectivement remplacés par **have** et **be**. La fonction `lemmatize` en est incapable car nous ne lui avons pas fourni les [étiquettes grammaticales](https://fr.wikipedia.org/wiki/Nature_(grammaire)) associées aux différents termes comme précédemment, en indiquant `wordnet.ADJ` pour **adjectif** ou `wordnet.NOUN` pour **nom**. Ces étiquettes grammaticales sont appelées [*Part Of Speech*](https://en.wikipedia.org/wiki/Part_of_speech) en anglais, d’où le nom du paramètre, `pos`. \n",
    "Qu'est-ce que wordnet ? [```WordNet```](https://en.wikipedia.org/wiki/WordNet)\n",
    "\n",
    "**<span style=\"color:red\">To do</span>**:\n",
    "\n",
    "> * Utilisez [```lemmatizer.lemmatize()```](https://www.machinelearningplus.com/nlp/lemmatization-examples-python/) pour la tokenisation, téléchargez les mots vides nltk avec ``python -m nltk.downloader stopwords`` et supprimez tous les autres **tokens** qui, selon vous, nuiront à la précision des algorithmes d'apprentissage automatique (nombres ? ponctuation ? Etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m nltk.downloader wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.pos_tag([\"This\", \"is\", \"an\", \"example\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\", pos=\"n\"))\n",
    "print(\"better : \", lemmatizer.lemmatize(\"better\", pos=wordnet.ADJ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:red\">To do</span>**:\n",
    "\n",
    "> * Ecrivez une méthode de prétraitement qui effectue également la lemmatisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_with_nltk_and_lemmatisation(phrase):\n",
    "    \n",
    "    tokens = []# YOUR CODE HERE\n",
    "    if len(tokens) > 1:\n",
    "        return tokens\n",
    "    return None\n",
    "\n",
    "data['cleaned_text_nltk_and_lemmatisation'] = data['text'].apply(preprocess_with_nltk_and_lemmatisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.2.3 Racinisation\n",
    "\n",
    "Un autre processus, la [**racinisation**](https://fr.wikipedia.org/wiki/Racinisation) (ou *stemming* en anglais) a une approche similaire. Cela consiste à ne conserver que la racine des mots étudiés : on supprime suffixes, préfixes et autres afin de ne conserver que la racine. C’est un procédé plus simple que la lemmatisation et plus rapide à effectuer puisque les mots sont tronqués contrairement à la lemmatisation qui nécessite d’utiliser un dictionnaire.\n",
    "\n",
    "En anglais, de tels dictionnaires existent, contrairement au français. Pour un texte en français, on utilisera plus simplement une **racinisation**, alors qu’en anglais, on pourra appliquer les deux traitements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import EnglishStemmer\n",
    "stemmer = EnglishStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La **racinisation** va tronquer les mots, par exemple comme ci-dessous :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"stayed :\", stemmer.stem(\"stayed\"))\n",
    "print(\"friends : \", stemmer.stem(\"friends\"))\n",
    "print(\"gaming :\", stemmer.stem(\"gaming\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:red\">To do</span>**:\n",
    "\n",
    "> * Ecrivez une méthode de prétraitement qui effectue également la racinisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_with_nltk_and_stemming(phrase):\n",
    "    \n",
    "    tokens = []# YOUR CODE HERE\n",
    "    if len(tokens) > 1:\n",
    "        return tokens\n",
    "    return None\n",
    "\n",
    "data['cleaned_text_nltk_and_stemming'] = data['text'].apply(preprocess_with_nltk_and_stemming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Representation des mots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4.1 « *bag of words* » (sac de mots)\n",
    "\n",
    "L’**extraction d’information** est l’étape qui suit le **nettoyage** du corpus. Pour ce faire, il nous faut changer la représentation du texte pour qu’il puisse être utilisé par un modèle statistique, et pour pouvoir l’exploiter.\n",
    "\n",
    "Dans les deux premières parties, nous avons représenté les corpus comme des **listes de mots ou lemmes ou racines**. Pour chaque document, nous avons représenté les termes dans un **sac**, sans prendre en compte le contexte de chaque terme. Dans ce **sac** est contenu un vocabulaire, le vocabulaire du document auquel on a associé le nombre d’occurence de chaque **lemme** dans le document. C’est ce que nous avons déjà fait à l’aide des deux normalisations que sont la **lemmatisation** et la **racinisation**.\n",
    "\n",
    "Une représentation **bag-of-words** classique sera donc celle dans laquelle on représente chaque document par un vecteur de la taille du vocabulaire $|V|$. On utilisera la matrice composée de l’ensemble de ces $N$ documents qui forment le corpus comme entrée de nos algorithmes.\n",
    "\n",
    "Nous pouvons construire ces vecteurs $V$ à l’aide de la classe [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) de `scikit-learn` :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ceci est juste un petit exemple avec les documents de 110 à 130 (problèmes de ressources mémoire)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2))\n",
    "vectorizer.fit(np.hstack(data['cleaned_text'][110:130]))\n",
    "\n",
    "X = vectorizer.transform(np.hstack(data['cleaned_text'][110:130]))\n",
    "\n",
    "pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names()).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:red\">To do</span>**:\n",
    "\n",
    "> * Écrivez le même code mais avec des gammes de mots n-grammes ou char n-grammes différents. Plus d'info [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = # YOUR CODE HERE\n",
    "vectorizer.fit(np.hstack(data['cleaned_text'])[110:130])\n",
    "\n",
    "X = vectorizer.transform(np.hstack(data['cleaned_text'])[110:130])\n",
    "\n",
    "pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names()).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Représenter un document par des *n-grammes* n’a d’intérêt que si l’on peut le faire relativement à leur apparition dans les autres documents. Nous recherchons là des *n-grammes* qui permettent de discriminer les documents entre eux, ceux qui confèrent une information utilie à la discrimination.\n",
    "\n",
    "Un lemme apparaissant dans un unique document dans un corpus est un lemme qui discrimine ce document et permet de l’identifier avec précision.\n",
    "\n",
    "Ce que nous désirons faire avec [**TF-IDF**](https://fr.wikipedia.org/wiki/TF-IDF) est de mettre en application ce qui est décrit précédemment. Nous voulons pondérer la fréquence d’apparition d’un lemme dans un document par rapport à son apparition dans l’ensemble des documents du corpus. La fréquence de chaque lemme est donc pondérée à l’ensemble du corpus.\n",
    "\n",
    "En l’occurence, la métrique **TF-IDF** (*Term-Frequency* − *Inverse Document Frequency*) utilise comme indicateur de similarité l’*inverse document frequency* qui est l’inverse de la proportion de document qui contient le terme, à l'échelle logarithmique.\n",
    "\n",
    "Nous calculons donc le poids **TF-IDF** final attribué au n-gramme :\n",
    "\n",
    "$tfidf_{i,j} = tf_{i,j} × idf_{i}$\n",
    "\n",
    "Pour connaître les termes qui représentent le plus un document, nous allons utiliser la [fonction **TF-IDF**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) de `scikit-learn`. **Attention aux valeurs par défaut lors de l’initialisation**.\n",
    "\n",
    "\n",
    "**Note :** nous traitons des *n-grames*, donc utilisez le paramètre [`ngram_range`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) de la classe [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) lors de son instanciation.\n",
    "\n",
    "**Note :** pour transformer vos vecteurs résultats (obtenus par appel à `fit_transform`) en des données utilisables dans un `DataFrame`, vous devez utiliser `vectors.toarray()`. À la création de votre `DataFrame`, nommez les colonnes avec le nom des lemmes utilisés (ce sont les *features* de l’opération, vous pouvez les récupérer avec `get_feature_names()`).\n",
    "\n",
    "**Note :** comment on renverse une matrice ? En demandant sa transposée. Pour la transposée d’un `pd.DataFrame`, on accède simplement l’attribut `T` du `pd.DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(np.hstack(data['cleaned_text'])[110:130])\n",
    "\n",
    "pd.DataFrame(vectors.toarray(), columns=vectorizer.get_feature_names()).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:red\">To do</span>**:\n",
    "\n",
    "> * Écrivez le même code mais avec des gammes de mots n-grammes ou char n-grammes différents. Plus d'info [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = # YOUR CODE HERE\n",
    "vectors = vectorizer.fit_transform(np.hstack(data['cleaned_text'])[110:130])\n",
    "\n",
    "pd.DataFrame(vectors.toarray(), columns=vectorizer.get_feature_names()).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 2 Plongements de mots (Word Embeddings)\n",
    "\n",
    "La dernière répresentation des mots que nous analyserons est la répresentation **word embeddings**. \n",
    "Les **plongements de mots** (ou *word embedding*) est une technique d'apprentissage et de représentation de mots d’un texte. Elle consiste à représenter les mots sous forme de vecteurs de nombres réels qui ont la particularité d’être proches (dans leurs espaces vectoriels) si leurs contextes sont similaires.\n",
    "\n",
    "Ils peuvent être générés de plusieurs façons, comme les [réseaux de neuronnes](https://www.tensorflow.org/tutorials/text/word_embeddings) ou les matrices de [cooccurences](https://fr.wikipedia.org/wiki/Cooccurrence), des modèles probabilistes (TF-IDF vecteurs), etc.\n",
    "\n",
    "Cette nouvelle représentation a ceci de particulier que les mots apparaissant dans des contextes similaires possèdent des vecteurs correspondants qui sont relativement proches. Par exemple, on pourrait s'attendre à ce que les mots « chien » et « chat » soient représentés par des vecteurs relativement peu distants dans l'espace vectoriel où sont définis ces vecteurs. Cette technique est basée sur l'hypothèse qui veut que les mots apparaissant dans des contextes similaires ont des significations apparentées. \n",
    "\n",
    "<figure>\n",
    "    <center>\n",
    "    <img src=\"images/we.png\" width=\"300\" height=\"400\">\n",
    "    <figcaption>Source: <a href=\"https://unbabel.com/blog/fr/lia-vous-parle-mais-comprend-elle-ce-quelle-dit/\"> link</a></figcaption>\n",
    "    </center>\n",
    "</figure>\n",
    "\n",
    "Différents  plongements (embeddings) de mots :\n",
    "\n",
    "*  [Collobert & Weston](http://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf) https://ronan.collobert.com/senna/\n",
    "* [Word2Vec Google News](https://code.google.com/archive/p/word2vec/)\n",
    "* [GloVe](https://nlp.stanford.edu/projects/glove/)\n",
    "* [FastText](https://github.com/facebookresearch/fastText)\n",
    "\n",
    "\n",
    "Le word embedding d'un mot peut être récupérée en utilisant la bibliothèque **spaCy**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = nlp(\"tobacco\")\n",
    "token, token.vector.shape, token.vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**spaCy** est capable de comparer deux mots et de faire une prédiction de leur similitude. La prédiction de la similarité est utile pour créer des systèmes de recommandation ou pour signaler les doublons. Par exemple, vous pouvez suggérer un contenu similaire à ce que l'utilisateur recherche actuellement ou étiqueter un ticket d'assistance comme doublon s'il est très similaire à un ticket déjà existant.\n",
    "\n",
    "Chaque **Doc**, **Span** et **Token** est livré avec une **.similarity()** méthode qui vous permet de le comparer avec un autre objet et de déterminer la similitude. Bien sûr, la similitude est toujours subjective - si «chien» et «chat» sont similaires dépend vraiment de la façon dont vous le regardez. Le modèle de similarité de **spaCy** suppose généralement une définition assez générale de la similitude.\n",
    "\n",
    "Généralement, ene fois que nous aurons des vecteurs du texte donné, pour calculer la similitude entre les vecteurs générés, des méthodes statistiques pour la similitude vectorielle peuvent être utilisées. Ces techniques sont la *similitude cosinus*, la *distance euclidienne*, la *distance de Jaccard*, la distance *word mover*. La *similitude cosinus* est la technique qui est largement utilisée pour la similitude de texte (**spaCy** utilise cette similitude).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = nlp('cigarettes')\n",
    "for word in ['cigarettes', 'health', 'cancer', 'capitalism', 'doctor', 'nurse', 'hospital', 'money', 'death']:\n",
    "    print('cigarettes', '~', word, ':', token.similarity(nlp(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour voir l'exemple de la figure présentée précédemment, nous calculons la distance entre tous les mots * spaCy * pour trouver les mots les plus similaires pour le calcul: **king − man + woman ≈ queen**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    " \n",
    "cosine_similarity = lambda x, y: 1 - spatial.distance.cosine(x, y)\n",
    " \n",
    "man = nlp.vocab['man'].vector\n",
    "woman = nlp.vocab['woman'].vector\n",
    "queen = nlp.vocab['queen'].vector\n",
    "king = nlp.vocab['king'].vector\n",
    " \n",
    "# Nous devons maintenant trouver le vecteur le plus proche du vocabulaire du résultat de \"man\" - \"woman\" + \"queen\"\n",
    "maybe_king = man - woman + queen\n",
    "computed_similarities = []\n",
    " \n",
    "for word in nlp.vocab:\n",
    "    # Ignorer les mots sans vecteurs\n",
    "    if not word.has_vector:\n",
    "        continue\n",
    " \n",
    "    similarity = cosine_similarity(maybe_king, word.vector)\n",
    "    computed_similarities.append((word, similarity))\n",
    "\n",
    "computed_similarities = sorted(computed_similarities, key=lambda item: -item[1])\n",
    "print([w[0].text for w in computed_similarities[:10]])\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Preparation pour apprentissage automatique (machine learning) et apprentissage profond (deep learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Diviser les jeux de données \n",
    "Afin d'entraîner des modèles pour apprentissage automatique et évaluer la performance de ses modèles avec chaque répresentation de mots, nous allons diviser les jeux de données en : entraînement, validation et test.\n",
    "\n",
    "Afin d'évaluer correctement la performance de chaque modèle, il est très important que les données d'entraînement et de test soient différentes. La bibliothèque <b>sklearn</b> a la fonction <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\"><i>train_test_split</i></a> qui divise un jeux de données. Le parametre <i>test_size</i> défini la taille du test (pourcentage) dans les jeux de données.\n",
    "\n",
    "Pour comprendre pourquoi nous devons diviser les données de cette manière, vous pouvez regarder Andrew Ng expliquer [ici](https://www.youtube.com/watch?v=1waHlpKiNyY). Ng est professeur à l'Université de Stanford et pionnier de l'éducation en ligne, Ng a cofondé [Coursera](https://www.coursera.org/learn/machine-learning) et [deeplearning.ai](https://www.deeplearning.ai/). Il a mené avec succès de nombreux efforts pour «démocratiser l'apprentissage profond» en enseignant plus de 2,5 millions d'étudiants grâce à ses cours en ligne. Il est l'un des informaticiens les plus connus et les plus influents au monde.\n",
    "\n",
    "Dans notre cas, nous irons fournir les phrases et les classes pour chaque phrase et la taille du jeu de données de test. Pour l'instant, on a pas besoin de donees de validation. On va voir de ca plus tard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour plus de simplicité, nous allons travailler uniquement avec des données pour entraînement et test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['text'], data['label'], test_size=0.2)\n",
    "len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Apprentissage automatique (machine learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objectif de la catégorisation de textes est de pouvoir associer automatiquement des documents à des classes (catégories, étiquettes, index) prédéfinies. Nous nous plaçons dans le cadre de l'apprentissage automatique supervisé. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Approaches de classification\n",
    "\n",
    "La classification naïve bayésienne est un type de classification bayésienne probabiliste simple basée sur le théorème de Bayes avec une forte indépendance des hypothèses. Elle met en œuvre un classifieur bayésien naïf, ou classifieur naïf de Bayes, appartenant à la famille des classifieurs linéaires. \n",
    "\n",
    "Un classifieur bayésien naïf suppose que l'existence d'une caractéristique pour une classe, est indépendante de l'existence d'autres caractéristiques. Un fruit peut être considéré comme une pomme s'il est rouge, arrondi, et fait une dizaine de centimètres. Même si ces caractéristiques sont liées dans la réalité, un classifieur bayésien naïf déterminera que le fruit est une pomme en considérant indépendamment ces caractéristiques de couleur, de forme et de taille. \n",
    "\n",
    "Le modèle probabiliste de cette approache est le modèle conditionnel $P(X | x_1,...,x_n)$, où $X$ est la variable « de classe » (celle qui indique si un individu appartient à une classe donnée) conditionnée par plusieurs variables caractéristiques $x_i$ (par exemple l’aile, le bec et cancane).\n",
    "\n",
    "Le théorème de Bayes s’énonce avec nos notations de la manière suivante :\n",
    "\n",
    "<center>\n",
    "$P(X|x_1, ..., x_n) = \\frac{P(x_1, ..., x_n | X)P(X)}{P(x_1, ..., x_n)}$\n",
    "</center>\n",
    "\n",
    "L'avantage du classifieur bayésien naïf est qu'il requiert relativement peu de données d'entraînement pour estimer les paramètres nécessaires à la classification, à savoir moyennes et variances des différentes variables.\n",
    "\n",
    "La bibliothèque sklearn fournit une classe <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\">MultinomialNB</a> qui nous permet d'entraîner et tester un modèle à partir d'un jeux de données d'entraînement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tout d'abord, nous représentons les documents en utilisant TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, nous utilisons les données d'entraînement pour entraîner le modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB()\n",
    "_ = clf.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après avoir entraîné notre modèle, nous pouvons faire la prediction des classes des messages du jeux de données de test.\n",
    " \n",
    "## Analyse d'erreurs\n",
    "\n",
    "Le taux d'erreur de classification donne une évaluation des performances pour toutes les classes. Mais comme les classes ne sont pas également réparties, elles peuvent ne pas être également bien modélisées. Afin d'avoir une meilleure idée des performances du classifieur, des métriques détaillées doivent être utilisées:\n",
    "\n",
    "* [metrics.classification_report](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) fournit une analyse détaillée par classe: la précision (parmi tous les exemples classés en classe X, combien sont réellement de la classeX) et le rappel (parmi tous les exemples qui sont de la classe X, combien sont classés en classe X) et le F-Score qui est une moyenne harmonique pondérée de la précision et du rappel.\n",
    "* [metrics.confusion_matrix](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) qui donnent les confusions entre les classes.\n",
    "\n",
    "F1-score = 2 x $\\frac{précision\\ x\\ rappel}{précision\\ +\\ rappel}$\n",
    "\n",
    " - précision : la proportion d'identifications positives était effectivement correcte.\n",
    " - rappel : la proportion de résultats positifs réels a été identifiée correctement. \n",
    "\n",
    "\n",
    "**<span style=\"color:red\">To do</span>**:\n",
    "\n",
    "> * Rapportez le `classification_report` pour votre classificateur. Quelles classes ont les meilleurs scores ? Pourquoi ?\n",
    "> * Rapportez la `confusion_matrix` pour votre classificateur. Quelles classes sont les plus confuses ? Pourquoi ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = clf.predict(X_test_tfidf)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, xticklabels=set(y_train), yticklabels=set(y_train))\n",
    "plt.ylabel('Vraies classes')\n",
    "plt.xlabel('Classes prédites')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les machines à vecteurs de support (en anglais support vector machine, SVM) sont un ensemble de techniques d'apprentissage supervisé destinées à résoudre des problèmes de discrimination et de régression. Ces techniques reposent sur deux idées clés : la notion de marge maximale et la notion de fonction noyau.\n",
    "\n",
    "La <b>marge</b> est la distance entre la frontière de séparation et les échantillons les plus proches. Dans les SVM, la frontière de séparation est choisie comme celle qui maximise la marge.\n",
    "\n",
    "<figure>\n",
    "    <center>\n",
    "    <img src=\"images/marge.png\" width=\"400\" height=\"400\">\n",
    "    <figcaption>Source: <a href=\"https://fr.wikipedia.org/wiki/Machine_%C3%A0_vecteurs_de_support\"> link</a></figcaption>\n",
    "    </center>\n",
    "</figure>\n",
    "\n",
    "La deuxième idée clé des SVM est de transformer l'espace de représentation des données d'entrées en un espace de plus grande dimension, dans lequel il est probable qu'il existe une séparation linéaire. Les fonctions <b>noyau</b> permettent de transformer un produit scalaire dans un espace de grande dimension, ce qui est coûteux, en une simple évaluation ponctuelle d'une fonction. Des noyaux usuels employés avec les SVM sont : le noyau polynomial et le noyau gaussien.\n",
    "\n",
    "<figure>\n",
    "    <center>\n",
    "    <img src=\"images/noyau.png\" width=\"400\" height=\"400\">\n",
    "    <figcaption>Source: <a href=\"https://fr.wikipedia.org/wiki/Machine_%C3%A0_vecteurs_de_support\"> link</a></figcaption>\n",
    "    </center>\n",
    "</figure>\n",
    "\n",
    "La bibliothèque sklearn propose un module pour utiliser de <a href=\"https://scikit-learn.org/stable/modules/svm.html\">SVMs</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:red\">To do</span>**:\n",
    "\n",
    "> * Entraînez, testez et faites l'évaluation du modèle SVM linéaire.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf = svm.LinearSVC()\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:red\">To do</span>**:\n",
    "\n",
    "> * Utilisez les autres méthodes de prétraitement (avec lemmatisation, avec racinisation, etc.) et comparez les résultats de ces deux modèles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Apprentissage profond (deep learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification avec des réseaux de neurones\n",
    "\n",
    "Les réseaux de neurones peuvent être entraînés pour apprendre à la fois la représentation vectorielle des mots (au lieu de tf-idf) et comment classer les documents. Le code ci-dessous vous permet d'entraîner un classificateur de texte neuronal à l'aide de l'intégration de mots à l'aide de Keras. La plupart du code est écrit, il suffit de définir l'architecture du réseau avec les bons paramètres avant de l'entraîner:\n",
    "\n",
    "**<span style=\"color:red\">To do</span>**:\n",
    "\n",
    "> * Aller plus loin. Vérifiez [Text classification Keras examples](https://keras.io/examples/nlp/)\n",
    "\n",
    "> * Définissez un réseau de neurones dans la fonction `get_model()` avec les paramètres suivants:\n",
    "> * n'utiliser que les 10000 mots les plus fréquents dans les documents MAX_FEATURES\n",
    "> * utiliser 1024 comme nombre maximal de mots dans les articles MAX_TEXT_LENGTH\n",
    "> * utiliser une taille de 300 pour les embeddings EMBEDDING_SIZE: [word embeddings](https://keras.io/layers/embeddings/)\n",
    "> * utiliser d'autres valeurs pour les ngram_filters filtres convolutifs: [couche convolutive 1D](https://keras.io/layers/convolutional/#conv1d)\n",
    "> * Ajoutez une couche Dropout à l'endroit indiqué avec le valeur 0,2 [Dropout](https://keras.io/api/layers/regularization_layers/dropout/)\n",
    "> * Former le modèle.\n",
    "\n",
    "> * Comment ce réseau de neurones se compare-t-il aux autres modèles? \n",
    "> * Quelle est la performance?\n",
    "> * Qu'apporte le changement de paramètres dans la performance?\n",
    "> * Utilisez des plongements pré-entraînés et chargez-les en tant que poids dans ce modèle (au lieu de ceux générés aléatoirement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import os\n",
    "from nn_utils import TrainingHistory\n",
    "from tensorflow.keras.layers import Dense, Embedding, Input\n",
    "from tensorflow.keras.layers import GRU, Dropout, MaxPooling1D, Conv1D, Flatten, GlobalMaxPooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "import itertools\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import (classification_report, \n",
    "                             precision_recall_fscore_support, \n",
    "                             accuracy_score)\n",
    "\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configurez les paramètres du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FEATURES = # YOUR CODE HERE\n",
    "MAX_TEXT_LENGTH = # YOUR CODE HERE\n",
    "EMBEDDING_SIZE  = # YOUR CODE HERE\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 10\n",
    "VALIDATION_SPLIT = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette methode transforme chaque texte des textes en une séquence d'entiers [Plus d'info](https://stackoverflow.com/questions/51956000/what-does-keras-tokenizer-method-exactly-do). Donc, il prend chaque mot dans le texte et le remplace par sa valeur entière correspondante du dictionnaire [word_index](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer). Le dictionnaire a été obtenu à partir de [fit_on_texts](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(train_raw_text, test_raw_text):\n",
    "    \n",
    "    tokenizer = text.Tokenizer(num_words=MAX_FEATURES)\n",
    "\n",
    "    tokenizer.fit_on_texts(list(train_raw_text))\n",
    "    \n",
    "    train_tokenized = tokenizer.texts_to_sequences(train_raw_text)\n",
    "    test_tokenized = tokenizer.texts_to_sequences(test_raw_text)\n",
    "    \n",
    "    return sequence.pad_sequences(train_tokenized, maxlen=MAX_TEXT_LENGTH), \\\n",
    "           sequence.pad_sequences(test_tokenized, maxlen=MAX_TEXT_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Advertisement' 'Email' 'Form' 'Letter' 'Memo' 'News' 'Note' 'Report'\n",
      " 'Resume' 'Scientific'] 10\n"
     ]
    }
   ],
   "source": [
    "# Obtenez la liste des différentes classes\n",
    "\n",
    "CLASSES_LIST = np.unique(data['label'])\n",
    "n_out = len(CLASSES_LIST)\n",
    "\n",
    "print(CLASSES_LIST, n_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "\n",
    "    inputs = Input(shape=(MAX_TEXT_LENGTH,))\n",
    "    model = Embedding(MAX_FEATURES, EMBEDDING_SIZE)(inputs)\n",
    "    # YOUR CODE HERE couche Dropout\n",
    "    \n",
    "    ngram_filters = [1, 2, 3] # YOUR CODE HERE Modifier les valeurs n-gramme\n",
    "    filters = [300]*3\n",
    "\n",
    "    convs = []\n",
    "    for kernel_size, filter_length in zip(ngram_filters, filters):\n",
    "        conv = Conv1D(filters=filter_length,\n",
    "                      kernel_size=kernel_size,\n",
    "                      padding='same',\n",
    "                      activation='relu')(model)\n",
    "    \n",
    "        convs.append(conv)\n",
    "\n",
    "    model = Concatenate()(convs)\n",
    "    model = GlobalMaxPooling1D()(model)    \n",
    "\n",
    "    outputs = Dense(n_out, activation=\"softmax\")(model)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette méthode est pour entraîner et tester le modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fit_predict(model, X_train, X_test, y, history):\n",
    "    \n",
    "    model.fit(X_train, y,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              epochs=EPOCHS, verbose=1,\n",
    "              validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "    return model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour que le modèle comprenne les étiquettes, elles doivent être sous forme entière. Ainsi, pour cela, il existe cette méthode: [LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir la chaîne de classe en index (entiers)\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(CLASSES_LIST)\n",
    "\n",
    "y_train_encoded = le.transform(y_train) \n",
    "y_test_encoded = le.transform(y_test) \n",
    "train_y_cat = to_categorical(y_train_encoded, n_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_encoded[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtenir les données textuelles dans le format correct pour le réseau neuronal\n",
    "x_vec_train, x_vec_test = get_train_test(X_train, X_test)\n",
    "len(x_vec_train), len(x_vec_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# définir la topologie du réseau neuronal\n",
    "model = get_model()\n",
    "\n",
    "# Définir la procédure d'entrainement\n",
    "history = TrainingHistory(x_vec_test, y_test_encoded, CLASSES_LIST)\n",
    "\n",
    "# Train and predict\n",
    "y_predicted = train_fit_predict(model, x_vec_train, x_vec_test, train_y_cat, history).argmax(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_encoded, y_predicted, target_names=CLASSES_LIST))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "et voilà !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
